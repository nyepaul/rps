"""Enhanced audit logging service with comprehensive data collection."""
import json
from datetime import datetime, timedelta
from typing import Optional, Dict, Any
from flask import request, has_request_context, session
from flask_login import current_user
from src.database.connection import db
import re
from user_agents import parse as parse_user_agent
from src.services.ip_intelligence import ip_intelligence


class AuditConfig:
    """Configuration for audit logging - what to collect and display."""

    # Default configuration
    DEFAULT_CONFIG = {
        'enabled': True,
        'collect': {
            'ip_address': True,  # Uses CF-Connecting-IP when behind Cloudflare
            'user_agent': True,
            'geo_location': True,  # Enhanced with CF-IPCountry when available
            'request_method': True,
            'request_endpoint': True,
            'request_headers': False,  # Privacy: off by default (CF metadata stored regardless)
            'request_body_size': True,
            'response_status': True,
            'session_id': True,
            'referrer': True,
            'device_info': True,
            'browser_info': True,
            'os_info': True,
            'login_attempts': True,
            'failed_actions': True,
            'cloudflare_metadata': True,  # CF-Ray, CF-IPCountry, CF-Cache-Status, etc.
            'browser_fingerprint': True,  # Language, encoding, screen size, timezone
            'risk_scoring': True,  # Security risk assessment (bot detection, automation tools)
            'session_metadata': True,  # Session age, size, authentication state
            'ip_intelligence': True  # IP analysis, VPN/proxy detection, reverse DNS
        },
        'display': {
            'ip_address': True,
            'user_agent': True,
            'geo_location': True,
            'request_method': True,
            'request_endpoint': True,
            'request_headers': False,  # Privacy: off by default
            'request_body_size': True,
            'response_status': True,
            'session_id': False,  # Privacy: hide by default in UI
            'referrer': True,
            'device_info': True,
            'browser_info': True,
            'os_info': True,
            'login_attempts': True,
            'failed_actions': True,
            'cloudflare_metadata': True,  # Show CF-Ray, CF-IPCountry, etc.
            'browser_fingerprint': True,  # Show language, screen size, etc.
            'risk_scoring': True,  # Show risk assessment
            'session_metadata': True,  # Show session details
            'ip_intelligence': True  # Show IP analysis, VPN detection
        },
        'retention_days': 90,
        'log_read_operations': False,  # Can generate lots of logs
        'sensitive_endpoints': [
            '/api/auth/login',
            '/api/auth/register',
            '/api/profiles',
            '/api/admin'
        ]
    }

    @staticmethod
    def get_config() -> Dict[str, Any]:
        """Get current audit configuration from database or default."""
        try:
            row = db.execute_one(
                'SELECT config_data FROM audit_config WHERE id = 1'
            )
            if row and row['config_data']:
                return json.loads(row['config_data'])
        except Exception:
            pass
        return AuditConfig.DEFAULT_CONFIG.copy()

    @staticmethod
    def set_config(config: Dict[str, Any]):
        """Save audit configuration to database."""
        config_json = json.dumps(config)
        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute('''
                INSERT OR REPLACE INTO audit_config (id, config_data, updated_at)
                VALUES (1, ?, ?)
            ''', (config_json, datetime.now().isoformat()))
            conn.commit()


class EnhancedAuditLogger:
    """Enhanced audit logger with comprehensive data collection."""

    @staticmethod
    def _get_geo_location(ip_address: str) -> Optional[Dict[str, str]]:
        """Get geographic location from IP address using ip-api.com."""
        if not ip_address or ip_address in ['127.0.0.1', 'localhost', '::1']:
            return {'city': 'Local', 'region': 'Local', 'country': 'Local', 'timezone': 'Local'}

        try:
            import requests
            # Using ip-api.com free tier (no API key required)
            response = requests.get(
                f'http://ip-api.com/json/{ip_address}?fields=status,country,countryCode,region,regionName,city,timezone',
                timeout=2
            )
            if response.status_code == 200:
                data = response.json()
                if data.get('status') == 'success':
                    return {
                        'country': data.get('country', 'Unknown'),
                        'country_code': data.get('countryCode', 'XX'),
                        'region': data.get('regionName', 'Unknown'),
                        'city': data.get('city', 'Unknown'),
                        'timezone': data.get('timezone', 'Unknown')
                    }
        except Exception as e:
            print(f"Geo-location lookup failed: {e}")

        return None

    @staticmethod
    def _parse_device_info(user_agent_string: str) -> Dict[str, str]:
        """Parse user agent string to extract device, browser, and OS info."""
        try:
            ua = parse_user_agent(user_agent_string)
            return {
                'browser': ua.browser.family,
                'browser_version': ua.browser.version_string,
                'os': ua.os.family,
                'os_version': ua.os.version_string,
                'device': ua.device.family,
                'device_brand': ua.device.brand or 'Unknown',
                'device_model': ua.device.model or 'Unknown',
                'is_mobile': ua.is_mobile,
                'is_tablet': ua.is_tablet,
                'is_pc': ua.is_pc,
                'is_bot': ua.is_bot
            }
        except Exception as e:
            print(f"User agent parsing failed: {e}")
            return {
                'browser': 'Unknown',
                'os': 'Unknown',
                'device': 'Unknown'
            }

    @staticmethod
    def _get_real_client_ip() -> str:
        """
        Get the real client IP address, checking Cloudflare headers first.

        When behind Cloudflare (or other proxies), request.remote_addr returns
        the proxy IP, not the real client IP. Cloudflare provides the real IP
        in the CF-Connecting-IP header.
        """
        if not has_request_context():
            return 'Unknown'

        # Priority order for IP detection:
        # 1. CF-Connecting-IP (Cloudflare's real client IP)
        # 2. X-Forwarded-For (standard proxy header, use first IP)
        # 3. X-Real-IP (alternative proxy header)
        # 4. request.remote_addr (fallback, may be proxy IP)

        # Check Cloudflare header first
        cf_ip = request.headers.get('CF-Connecting-IP')
        if cf_ip:
            return cf_ip

        # Check X-Forwarded-For (may contain chain of IPs, use the first one)
        x_forwarded_for = request.headers.get('X-Forwarded-For')
        if x_forwarded_for:
            # Take the first IP in the chain (original client)
            return x_forwarded_for.split(',')[0].strip()

        # Check X-Real-IP
        x_real_ip = request.headers.get('X-Real-IP')
        if x_real_ip:
            return x_real_ip

        # Fallback to remote_addr (may be proxy IP if behind reverse proxy)
        return request.remote_addr or 'Unknown'

    @staticmethod
    def _get_cloudflare_metadata() -> Optional[Dict[str, str]]:
        """
        Extract Cloudflare-specific metadata from request headers.

        Returns dict with Cloudflare metadata if headers are present, None otherwise.
        Cloudflare provides valuable tracking data in custom headers.
        """
        if not has_request_context():
            return None

        cf_metadata = {}

        # CF-Ray: Unique request identifier for Cloudflare support/debugging
        cf_ray = request.headers.get('CF-Ray')
        if cf_ray:
            cf_metadata['cf_ray'] = cf_ray

        # CF-IPCountry: Two-letter country code of client
        cf_country = request.headers.get('CF-IPCountry')
        if cf_country:
            cf_metadata['cf_country'] = cf_country

        # CF-Visitor: JSON with scheme info (http/https)
        cf_visitor = request.headers.get('CF-Visitor')
        if cf_visitor:
            try:
                import json as json_lib
                visitor_data = json_lib.loads(cf_visitor)
                cf_metadata['cf_scheme'] = visitor_data.get('scheme', 'unknown')
            except:
                cf_metadata['cf_visitor'] = cf_visitor

        # CF-Connecting-IP: Real client IP (already captured separately)
        cf_ip = request.headers.get('CF-Connecting-IP')
        if cf_ip:
            cf_metadata['cf_connecting_ip'] = cf_ip

        # CF-Request-ID: Cloudflare internal request ID
        cf_request_id = request.headers.get('CF-Request-ID')
        if cf_request_id:
            cf_metadata['cf_request_id'] = cf_request_id

        # CF-Cache-Status: Cache hit/miss status
        cf_cache_status = request.headers.get('CF-Cache-Status')
        if cf_cache_status:
            cf_metadata['cf_cache_status'] = cf_cache_status

        # CDN-Loop: Cloudflare edge location identifier
        cdn_loop = request.headers.get('CDN-Loop')
        if cdn_loop:
            cf_metadata['cdn_loop'] = cdn_loop

        return cf_metadata if cf_metadata else None

    @staticmethod
    def _get_browser_fingerprint() -> Dict[str, Any]:
        """
        Generate browser fingerprint for tracking and security.
        Combines multiple attributes to uniquely identify a browser/device.
        """
        if not has_request_context():
            return {}

        fingerprint = {}

        # User Agent
        ua_string = request.headers.get('User-Agent', '')
        if ua_string:
            fingerprint['user_agent_hash'] = hash(ua_string) % 1000000  # Shorter hash for storage

        # Language preferences (can indicate location/settings)
        accept_language = request.headers.get('Accept-Language', '')
        if accept_language:
            # Parse primary language
            primary_lang = accept_language.split(',')[0].strip() if accept_language else 'unknown'
            fingerprint['primary_language'] = primary_lang

        # Encoding preferences
        accept_encoding = request.headers.get('Accept-Encoding', '')
        fingerprint['supports_compression'] = 'gzip' in accept_encoding or 'br' in accept_encoding

        # Connection type hints
        connection = request.headers.get('Connection', '')
        fingerprint['connection_type'] = connection if connection else 'unknown'

        # DNT (Do Not Track) header
        dnt = request.headers.get('DNT', '0')
        fingerprint['dnt_enabled'] = dnt == '1'

        # Screen hints from client (if available in custom headers)
        screen_width = request.headers.get('X-Screen-Width')
        screen_height = request.headers.get('X-Screen-Height')
        if screen_width and screen_height:
            fingerprint['screen_resolution'] = f"{screen_width}x{screen_height}"

        # Timezone offset (if available in custom headers)
        tz_offset = request.headers.get('X-Timezone-Offset')
        if tz_offset:
            fingerprint['timezone_offset'] = tz_offset

        # Viewport size (if available)
        viewport_width = request.headers.get('X-Viewport-Width')
        viewport_height = request.headers.get('X-Viewport-Height')
        if viewport_width and viewport_height:
            fingerprint['viewport_size'] = f"{viewport_width}x{viewport_height}"

        return fingerprint

    @staticmethod
    def _calculate_risk_score(ip_address: str, user_agent: str, device_info: Dict) -> Dict[str, Any]:
        """
        Calculate basic risk indicators for the request.
        Returns dict with risk score and factors.
        """
        risk_indicators = {
            'score': 0,  # 0-100 scale
            'factors': []
        }

        # Check for bot
        if device_info.get('is_bot'):
            risk_indicators['score'] += 30
            risk_indicators['factors'].append('bot_detected')

        # Check for missing/suspicious user agent
        if not user_agent or len(user_agent) < 10:
            risk_indicators['score'] += 20
            risk_indicators['factors'].append('suspicious_user_agent')

        # Check for localhost/internal IP
        if ip_address in ['127.0.0.1', 'localhost', '::1'] or ip_address.startswith('192.168.') or ip_address.startswith('10.'):
            # Internal IPs are lower risk in some contexts
            risk_indicators['factors'].append('internal_network')

        # Check for Tor exit nodes (common IPs)
        # This is a simplified check - production would use a Tor exit node list
        if any(indicator in user_agent.lower() for indicator in ['tor', 'onion']):
            risk_indicators['score'] += 40
            risk_indicators['factors'].append('tor_browser')

        # Check for automation tools
        automation_keywords = ['selenium', 'puppeteer', 'phantom', 'headless', 'crawler', 'spider', 'bot']
        if any(keyword in user_agent.lower() for keyword in automation_keywords):
            risk_indicators['score'] += 25
            risk_indicators['factors'].append('automation_tool')

        # Cap score at 100
        risk_indicators['score'] = min(risk_indicators['score'], 100)

        # Assign risk level
        if risk_indicators['score'] >= 70:
            risk_indicators['level'] = 'high'
        elif risk_indicators['score'] >= 40:
            risk_indicators['level'] = 'medium'
        else:
            risk_indicators['level'] = 'low'

        return risk_indicators

    @staticmethod
    def _get_session_metadata() -> Dict[str, Any]:
        """Extract detailed session information."""
        if not has_request_context() or not session:
            return {}

        metadata = {}

        # Session age (if _created timestamp exists)
        if '_created' in session:
            try:
                created_at = datetime.fromisoformat(session['_created'])
                age_seconds = (datetime.now() - created_at).total_seconds()
                metadata['session_age_seconds'] = int(age_seconds)
                metadata['session_age_minutes'] = round(age_seconds / 60, 2)
            except Exception:
                pass

        # Session data size (approximate)
        try:
            session_data_size = len(str(dict(session)))
            metadata['session_size_bytes'] = session_data_size
        except Exception:
            pass

        # User authentication state
        if current_user and current_user.is_authenticated:
            metadata['authenticated'] = True
            metadata['user_id'] = current_user.id
            metadata['username'] = getattr(current_user, 'username', 'unknown')
            metadata['is_admin'] = getattr(current_user, 'is_admin', False)
        else:
            metadata['authenticated'] = False

        return metadata

    @staticmethod
    def _get_request_info() -> Dict[str, Any]:
        """Extract comprehensive request information."""
        if not has_request_context():
            return {}

        info = {
            'method': request.method,
            'endpoint': request.endpoint,
            'path': request.path,
            'query_string': request.query_string.decode('utf-8'),
            'referrer': request.referrer or 'Direct',
            'content_length': request.content_length or 0,
            'is_secure': request.is_secure,
            'scheme': request.scheme,
            'content_type': request.content_type or 'unknown',
        }

        # Session ID (first 8 chars for privacy)
        if session:
            try:
                sid = session.get('_id', 'N/A')
                info['session_id'] = sid[:8] if sid != 'N/A' else 'N/A'
            except Exception:
                info['session_id'] = 'N/A'

        # Selected headers (be careful with privacy)
        info['headers'] = {
            'accept': request.headers.get('Accept', 'N/A'),
            'accept_language': request.headers.get('Accept-Language', 'N/A'),
            'accept_encoding': request.headers.get('Accept-Encoding', 'N/A'),
            'origin': request.headers.get('Origin', 'N/A'),
            'dnt': request.headers.get('DNT', 'N/A'),
            'sec_fetch_site': request.headers.get('Sec-Fetch-Site', 'N/A'),
            'sec_fetch_mode': request.headers.get('Sec-Fetch-Mode', 'N/A'),
            'sec_fetch_dest': request.headers.get('Sec-Fetch-Dest', 'N/A'),
        }

        # Request timing hints
        if request.environ:
            info['protocol'] = request.environ.get('SERVER_PROTOCOL', 'unknown')

        return info

    @staticmethod
    def log(
        action: str,
        table_name: str = None,
        record_id: Optional[int] = None,
        user_id: Optional[int] = None,
        details: Optional[str] = None,
        status_code: Optional[int] = None,
        error_message: Optional[str] = None
    ):
        """
        Log an audit event with comprehensive data collection.

        Args:
            action: Type of action (CREATE, READ, UPDATE, DELETE, LOGIN, LOGOUT, etc.)
            table_name: Name of the table affected (if applicable)
            record_id: ID of the record affected (if applicable)
            user_id: ID of the user performing the action
            details: Additional details about the action (string or dict)
            status_code: HTTP status code of the response
            error_message: Error message if action failed
        """
        config = AuditConfig.get_config()

        # Check if logging is enabled
        if not config.get('enabled', True):
            return

        # Get user_id from current_user if not provided
        if user_id is None and has_request_context():
            try:
                if current_user and current_user.is_authenticated:
                    user_id = current_user.id
            except Exception:
                pass

        # Initialize audit data
        audit_data = {
            'action': action,
            'table_name': table_name,
            'record_id': record_id,
            'user_id': user_id,
            'details': details if isinstance(details, str) else json.dumps(details) if details else None,
            'status_code': status_code,
            'error_message': error_message,
            'created_at': datetime.now().isoformat()
        }

        if not has_request_context():
            # No request context, just save basic info
            EnhancedAuditLogger._save_audit_log(audit_data)
            return

        collect_config = config.get('collect', {})

        # Collect real client IP address (checks Cloudflare headers)
        if collect_config.get('ip_address', True):
            audit_data['ip_address'] = EnhancedAuditLogger._get_real_client_ip()

        # Collect Cloudflare metadata
        cf_metadata = EnhancedAuditLogger._get_cloudflare_metadata()

        # Collect user agent
        user_agent_string = request.headers.get('User-Agent', '')
        device_info = {}
        if collect_config.get('user_agent', True):
            audit_data['user_agent'] = user_agent_string[:500]  # Limit length

            # Parse device info from user agent
            if collect_config.get('device_info', True) or collect_config.get('browser_info', True):
                device_info = EnhancedAuditLogger._parse_device_info(user_agent_string)
                audit_data['device_info'] = json.dumps(device_info)

        # Collect geo location
        if collect_config.get('geo_location', True) and audit_data.get('ip_address'):
            geo_data = EnhancedAuditLogger._get_geo_location(audit_data['ip_address'])

            # Enhance with Cloudflare country data if available
            if geo_data and cf_metadata and 'cf_country' in cf_metadata:
                geo_data['cf_country_code'] = cf_metadata['cf_country']

            if geo_data:
                audit_data['geo_location'] = json.dumps(geo_data)

        # Perform IP intelligence analysis
        if collect_config.get('ip_intelligence', True) and audit_data.get('ip_address'):
            ip_analysis = ip_intelligence.analyze_ip(audit_data['ip_address'])
            vpn_detection = ip_intelligence.detect_vpn_proxy(
                audit_data['ip_address'],
                user_agent_string
            )

            # Store IP intelligence in details
            ip_intel_data = {
                'ip_analysis': ip_analysis,
                'vpn_detection': vpn_detection
            }

            # Add to details
            if audit_data.get('details'):
                try:
                    details_dict = json.loads(audit_data['details']) if isinstance(audit_data['details'], str) else {}
                    details_dict['ip_intelligence'] = ip_intel_data
                    audit_data['details'] = json.dumps(details_dict)
                except:
                    pass
            else:
                audit_data['details'] = json.dumps({'ip_intelligence': ip_intel_data})

        # Collect request information
        if collect_config.get('request_method', True) or collect_config.get('request_endpoint', True):
            request_info = EnhancedAuditLogger._get_request_info()
            audit_data['request_method'] = request_info.get('method')
            audit_data['request_endpoint'] = request_info.get('path')
            audit_data['request_query'] = request_info.get('query_string')

            if collect_config.get('referrer', True):
                audit_data['referrer'] = request_info.get('referrer')

            if collect_config.get('request_body_size', True):
                audit_data['request_size'] = request_info.get('content_length', 0)

            if collect_config.get('session_id', True):
                audit_data['session_id'] = request_info.get('session_id')

            if collect_config.get('request_headers', False):
                headers_data = request_info.get('headers', {})
                # Always include Cloudflare metadata in headers (even if request_headers is off)
                # This ensures we capture CF-Ray, CF-IPCountry, etc.
                if cf_metadata:
                    headers_data['cloudflare'] = cf_metadata
                audit_data['request_headers'] = json.dumps(headers_data)
            elif cf_metadata:
                # If request_headers collection is off but we have CF data,
                # store CF metadata separately
                audit_data['request_headers'] = json.dumps({'cloudflare': cf_metadata})

        # Collect browser fingerprint for tracking and security
        if collect_config.get('browser_fingerprint', True):
            fingerprint = EnhancedAuditLogger._get_browser_fingerprint()
            if fingerprint:
                # Store in device_info if it exists, otherwise create new field
                if audit_data.get('device_info'):
                    try:
                        existing_device_info = json.loads(audit_data['device_info'])
                        existing_device_info['fingerprint'] = fingerprint
                        audit_data['device_info'] = json.dumps(existing_device_info)
                    except:
                        pass
                else:
                    audit_data['device_info'] = json.dumps({'fingerprint': fingerprint})

        # Calculate risk score for security monitoring
        if collect_config.get('risk_scoring', True):
            ip_address = audit_data.get('ip_address', '')
            risk_data = EnhancedAuditLogger._calculate_risk_score(ip_address, user_agent_string, device_info)
            if risk_data:
                # Store risk data in details if it exists
                if audit_data.get('details'):
                    try:
                        details_dict = json.loads(audit_data['details']) if isinstance(audit_data['details'], str) else {}
                        details_dict['risk_assessment'] = risk_data
                        audit_data['details'] = json.dumps(details_dict)
                    except:
                        pass

        # Collect detailed session metadata
        if collect_config.get('session_metadata', True):
            session_meta = EnhancedAuditLogger._get_session_metadata()
            if session_meta:
                # Store in details
                if audit_data.get('details'):
                    try:
                        details_dict = json.loads(audit_data['details']) if isinstance(audit_data['details'], str) else {}
                        details_dict['session_metadata'] = session_meta
                        audit_data['details'] = json.dumps(details_dict)
                    except:
                        # If details is a string and can't be parsed, wrap it
                        details_dict = {
                            'original_details': audit_data['details'],
                            'session_metadata': session_meta
                        }
                        audit_data['details'] = json.dumps(details_dict)
                else:
                    audit_data['details'] = json.dumps({'session_metadata': session_meta})

        # Save to database
        EnhancedAuditLogger._save_audit_log(audit_data)

    @staticmethod
    def _save_audit_log(audit_data: Dict[str, Any]):
        """Save audit log entry to database."""
        try:
            with db.get_connection() as conn:
                cursor = conn.cursor()

                # Build dynamic SQL based on available fields
                fields = list(audit_data.keys())
                placeholders = ', '.join(['?' for _ in fields])
                field_names = ', '.join(fields)

                cursor.execute(f'''
                    INSERT INTO enhanced_audit_log ({field_names})
                    VALUES ({placeholders})
                ''', tuple(audit_data.values()))

                conn.commit()
        except Exception as e:
            # Don't let audit logging failures break the application
            print(f"Enhanced audit logging failed: {e}")

    @staticmethod
    def log_login_attempt(username: str, success: bool, ip_address: str = None, error_message: str = None):
        """Log a login attempt (success or failure)."""
        EnhancedAuditLogger.log(
            action='LOGIN_ATTEMPT',
            table_name='users',
            details=json.dumps({
                'username': username,
                'success': success
            }),
            status_code=200 if success else 401,
            error_message=error_message
        )

    @staticmethod
    def log_create(table_name: str, record_id: int, user_id: Optional[int] = None, details: Optional[str] = None):
        """Log a CREATE operation."""
        EnhancedAuditLogger.log('CREATE', table_name, record_id, user_id, details, status_code=201)

    @staticmethod
    def log_read(table_name: str, record_id: Optional[int] = None, user_id: Optional[int] = None, details: Optional[str] = None):
        """Log a READ operation."""
        config = AuditConfig.get_config()
        if not config.get('log_read_operations', False):
            return  # Skip READ operations if not configured
        EnhancedAuditLogger.log('READ', table_name, record_id, user_id, details, status_code=200)

    @staticmethod
    def log_update(table_name: str, record_id: int, user_id: Optional[int] = None, details: Optional[str] = None):
        """Log an UPDATE operation."""
        EnhancedAuditLogger.log('UPDATE', table_name, record_id, user_id, details, status_code=200)

    @staticmethod
    def log_delete(table_name: str, record_id: int, user_id: Optional[int] = None, details: Optional[str] = None):
        """Log a DELETE operation."""
        EnhancedAuditLogger.log('DELETE', table_name, record_id, user_id, details, status_code=200)

    @staticmethod
    def log_admin_action(action: str, details: Optional[Dict] = None, user_id: Optional[int] = None):
        """Log an admin action."""
        EnhancedAuditLogger.log(
            action=f'ADMIN_{action}',
            table_name='admin',
            user_id=user_id,
            details=details,
            status_code=200
        )

    @staticmethod
    def log_data_change(
        action: str,
        table_name: str,
        record_id: int,
        old_data: Optional[Dict] = None,
        new_data: Optional[Dict] = None,
        user_id: Optional[int] = None
    ):
        """
        Log a data change operation with before/after snapshots.

        Args:
            action: CREATE, UPDATE, or DELETE
            table_name: Name of the table
            record_id: ID of the record
            old_data: Data before the change (for UPDATE/DELETE)
            new_data: Data after the change (for CREATE/UPDATE)
            user_id: User making the change
        """
        details = {
            'change_tracking': {
                'old_data': old_data,
                'new_data': new_data
            }
        }

        # Calculate what changed
        if old_data and new_data:
            changes = {}
            all_keys = set(old_data.keys()) | set(new_data.keys())
            for key in all_keys:
                old_val = old_data.get(key)
                new_val = new_data.get(key)
                if old_val != new_val:
                    changes[key] = {
                        'old': old_val,
                        'new': new_val
                    }
            details['change_tracking']['changed_fields'] = changes
            details['change_tracking']['num_fields_changed'] = len(changes)

        EnhancedAuditLogger.log(
            action=action,
            table_name=table_name,
            record_id=record_id,
            user_id=user_id,
            details=details,
            status_code=200
        )

    @staticmethod
    def log_performance(
        action: str,
        duration_ms: float,
        details: Optional[Dict] = None,
        status_code: int = 200
    ):
        """
        Log performance metrics for an operation.

        Args:
            action: Name of the operation
            duration_ms: Duration in milliseconds
            details: Additional context
            status_code: HTTP status code
        """
        perf_details = {
            'performance': {
                'duration_ms': round(duration_ms, 2),
                'duration_seconds': round(duration_ms / 1000, 3)
            }
        }

        # Classify performance
        if duration_ms < 100:
            perf_details['performance']['classification'] = 'fast'
        elif duration_ms < 500:
            perf_details['performance']['classification'] = 'normal'
        elif duration_ms < 2000:
            perf_details['performance']['classification'] = 'slow'
        else:
            perf_details['performance']['classification'] = 'very_slow'

        # Merge with existing details
        if details:
            perf_details.update(details)

        EnhancedAuditLogger.log(
            action=action,
            details=perf_details,
            status_code=status_code
        )

    @staticmethod
    def detect_suspicious_patterns(user_id: Optional[int] = None, ip_address: Optional[str] = None, minutes: int = 5):
        """
        Detect suspicious patterns in recent audit logs.
        Returns list of detected patterns.

        Args:
            user_id: Filter by user ID
            ip_address: Filter by IP address
            minutes: Look back window in minutes
        """
        cutoff = (datetime.now() - timedelta(minutes=minutes)).isoformat()
        patterns = []

        # Query recent logs
        query = 'SELECT * FROM enhanced_audit_log WHERE created_at >= ?'
        params = [cutoff]

        if user_id:
            query += ' AND user_id = ?'
            params.append(user_id)

        if ip_address:
            query += ' AND ip_address = ?'
            params.append(ip_address)

        logs = db.execute(query, tuple(params))
        log_list = [dict(row) for row in logs]

        # Pattern 1: High frequency of failed logins
        failed_logins = [log for log in log_list if log['action'] == 'LOGIN_ATTEMPT' and log.get('status_code') == 401]
        if len(failed_logins) >= 5:
            patterns.append({
                'type': 'brute_force_attempt',
                'severity': 'high',
                'count': len(failed_logins),
                'description': f'{len(failed_logins)} failed login attempts in {minutes} minutes'
            })

        # Pattern 2: Rapid API calls (possible scraping/automation)
        if len(log_list) >= 100:
            patterns.append({
                'type': 'high_frequency_requests',
                'severity': 'medium',
                'count': len(log_list),
                'description': f'{len(log_list)} requests in {minutes} minutes'
            })

        # Pattern 3: Multiple failed actions
        failed_actions = [log for log in log_list if log.get('status_code', 200) >= 400]
        if len(failed_actions) >= 10:
            patterns.append({
                'type': 'repeated_failures',
                'severity': 'medium',
                'count': len(failed_actions),
                'description': f'{len(failed_actions)} failed actions in {minutes} minutes'
            })

        # Pattern 4: Access to many different endpoints (reconnaissance)
        unique_endpoints = set(log.get('request_endpoint') for log in log_list if log.get('request_endpoint'))
        if len(unique_endpoints) >= 20:
            patterns.append({
                'type': 'endpoint_scanning',
                'severity': 'high',
                'count': len(unique_endpoints),
                'description': f'Access to {len(unique_endpoints)} different endpoints in {minutes} minutes'
            })

        return patterns

    @staticmethod
    def analyze_fingerprint_data(fingerprint_data: Dict, ip_geolocation: Dict = None) -> Dict[str, Any]:
        """
        Analyze browser fingerprint data for anomalies and security insights.

        Returns dict with:
        - consistency_score: 0-100 (higher is more consistent/trustworthy)
        - anomalies: List of detected anomalies
        - device_profile: Summary of device characteristics
        - location_mismatch: Timezone/language vs IP location mismatch analysis
        """
        analysis = {
            'consistency_score': 100,
            'anomalies': [],
            'device_profile': {},
            'risk_factors': []
        }

        if not fingerprint_data:
            return analysis

        # Analyze basic info
        basic = fingerprint_data.get('basic', {})

        # Check for missing/suspicious user agent
        if not basic.get('user_agent') or len(basic.get('user_agent', '')) < 10:
            analysis['anomalies'].append('missing_or_suspicious_user_agent')
            analysis['consistency_score'] -= 20

        # Check for automation indicators
        if basic.get('webdriver'):
            analysis['anomalies'].append('webdriver_detected')
            analysis['risk_factors'].append('automation')
            analysis['consistency_score'] -= 30

        # Check headless browser indicators
        if basic.get('product_sub') == '20030107':  # Headless Chrome indicator
            analysis['anomalies'].append('headless_browser_indicator')
            analysis['risk_factors'].append('automation')
            analysis['consistency_score'] -= 25

        # Analyze screen info
        screen = fingerprint_data.get('screen', {})
        if screen:
            # Unusual screen resolutions (potential emulation)
            width = screen.get('width', 0)
            height = screen.get('height', 0)
            if width == height:  # Square screens are uncommon
                analysis['anomalies'].append('unusual_screen_resolution')
                analysis['consistency_score'] -= 10

            # Check for common emulator resolutions
            emulator_resolutions = [(360, 640), (375, 667), (414, 896), (1920, 1080)]
            if (width, height) in emulator_resolutions:
                # Common resolutions, but could indicate emulation
                analysis['device_profile']['likely_emulated'] = True

        # Analyze capabilities
        capabilities = fingerprint_data.get('capabilities', {})
        if capabilities:
            # Check for impossible combinations
            if capabilities.get('touch_support') and not basic.get('max_touch_points'):
                analysis['anomalies'].append('touch_capability_mismatch')
                analysis['consistency_score'] -= 15

        # Analyze timezone and location consistency
        if ip_geolocation:
            timezone_info = fingerprint_data.get('timezone', {})
            mismatch = ip_intelligence.analyze_ip_location_mismatch(
                ip_geolocation,
                timezone_info.get('timezone'),
                basic.get('language')
            )

            if mismatch.get('has_mismatch'):
                analysis['location_mismatch'] = mismatch
                analysis['consistency_score'] -= mismatch.get('confidence', 0) // 2
                analysis['risk_factors'].append('location_mismatch')

        # Device profile summary
        analysis['device_profile'] = {
            'platform': basic.get('platform'),
            'hardware_concurrency': basic.get('hardware_concurrency'),
            'device_memory': basic.get('device_memory'),
            'screen_resolution': f"{screen.get('width')}x{screen.get('height')}",
            'color_depth': screen.get('color_depth'),
            'touch_capable': basic.get('max_touch_points', 0) > 0,
            'webgl_vendor': fingerprint_data.get('webgl', {}).get('vendor'),
            'canvas_hash': fingerprint_data.get('canvas', {}).get('hash'),
            'composite_fingerprint': fingerprint_data.get('composite_fingerprint')
        }

        # Overall risk assessment
        if analysis['consistency_score'] < 50:
            analysis['risk_level'] = 'high'
        elif analysis['consistency_score'] < 70:
            analysis['risk_level'] = 'medium'
        else:
            analysis['risk_level'] = 'low'

        return analysis

    @staticmethod
    def get_logs(
        user_id: Optional[int] = None,
        action: Optional[str] = None,
        table_name: Optional[str] = None,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        ip_address: Optional[str] = None,
        limit: int = 100,
        offset: int = 0,
        sort_by: str = 'created_at',
        sort_direction: str = 'desc'
    ):
        """
        Retrieve audit logs with comprehensive filtering.

        Returns both the logs and total count for pagination.

        Args:
            sort_by: Column to sort by (default: created_at)
            sort_direction: Sort direction 'asc' or 'desc' (default: desc)
        """
        config = AuditConfig.get_config()
        display_config = config.get('display', {})

        # Build query with JOIN to get username
        query = '''
            SELECT
                eal.*,
                u.username
            FROM enhanced_audit_log eal
            LEFT JOIN users u ON eal.user_id = u.id
            WHERE 1=1
        '''
        count_query = 'SELECT COUNT(*) as total FROM enhanced_audit_log WHERE 1=1'
        params = []

        if user_id is not None:
            # Handle special case for filtering NULL user_id (unauthenticated users)
            if user_id == 'null':
                query += ' AND user_id IS NULL'
                count_query += ' AND user_id IS NULL'
            else:
                query += ' AND user_id = ?'
                count_query += ' AND user_id = ?'
                params.append(user_id)

        if action is not None:
            query += ' AND action = ?'
            count_query += ' AND action = ?'
            params.append(action)

        if table_name is not None:
            query += ' AND table_name = ?'
            count_query += ' AND table_name = ?'
            params.append(table_name)

        if start_date is not None:
            query += ' AND created_at >= ?'
            count_query += ' AND created_at >= ?'
            params.append(start_date)

        if end_date is not None:
            query += ' AND created_at <= ?'
            count_query += ' AND created_at <= ?'
            params.append(end_date)

        if ip_address is not None:
            query += ' AND ip_address = ?'
            count_query += ' AND ip_address = ?'
            params.append(ip_address)

        # Get total count
        count_result = db.execute_one(count_query, tuple(params))
        total_count = count_result['total'] if count_result else 0

        # Whitelist allowed sort columns to prevent SQL injection
        # Include all columns from enhanced_audit_log table plus username from JOIN
        allowed_sort_columns = [
            'id', 'created_at', 'action', 'table_name', 'record_id',
            'user_id', 'username', 'details', 'status_code', 'error_message',
            'ip_address', 'user_agent', 'geo_location', 'device_info',
            'request_method', 'request_endpoint', 'request_query',
            'request_size', 'referrer', 'session_id', 'request_headers'
        ]
        if sort_by not in allowed_sort_columns:
            sort_by = 'created_at'

        # Validate sort direction
        sort_direction = 'ASC' if sort_direction.lower() == 'asc' else 'DESC'

        # Get paginated results with dynamic sorting
        # Handle username from JOIN separately (u.username vs eal.column)
        if sort_by == 'username':
            query += f' ORDER BY u.{sort_by} {sort_direction} LIMIT ? OFFSET ?'
        else:
            query += f' ORDER BY eal.{sort_by} {sort_direction} LIMIT ? OFFSET ?'
        params.extend([limit, offset])

        rows = db.execute(query, tuple(params))
        logs = [dict(row) for row in rows]

        # Filter fields based on display configuration
        filtered_logs = []
        for log in logs:
            filtered_log = {
                'id': log.get('id'),
                'action': log.get('action'),
                'table_name': log.get('table_name'),
                'record_id': log.get('record_id'),
                'user_id': log.get('user_id'),
                'username': log.get('username'),  # Include username from JOIN
                'created_at': log.get('created_at'),
                'status_code': log.get('status_code'),
                'error_message': log.get('error_message')
            }

            # Add fields based on display configuration
            if display_config.get('ip_address', True):
                filtered_log['ip_address'] = log.get('ip_address')

            if display_config.get('user_agent', True):
                filtered_log['user_agent'] = log.get('user_agent')

            if display_config.get('geo_location', True):
                geo_str = log.get('geo_location')
                if geo_str:
                    try:
                        filtered_log['geo_location'] = json.loads(geo_str)
                    except:
                        pass

            if display_config.get('device_info', True):
                device_str = log.get('device_info')
                if device_str:
                    try:
                        filtered_log['device_info'] = json.loads(device_str)
                    except:
                        pass

            if display_config.get('request_method', True):
                filtered_log['request_method'] = log.get('request_method')

            if display_config.get('request_endpoint', True):
                filtered_log['request_endpoint'] = log.get('request_endpoint')

            if display_config.get('referrer', True):
                filtered_log['referrer'] = log.get('referrer')

            if display_config.get('session_id', True):
                filtered_log['session_id'] = log.get('session_id')

            # Extract Cloudflare metadata from request_headers if enabled
            if display_config.get('cloudflare_metadata', True):
                headers_str = log.get('request_headers')
                if headers_str:
                    try:
                        headers_data = json.loads(headers_str)
                        cf_data = headers_data.get('cloudflare')
                        if cf_data:
                            filtered_log['cloudflare'] = cf_data
                    except:
                        pass

            # Always include details
            details_str = log.get('details')
            if details_str:
                try:
                    filtered_log['details'] = json.loads(details_str)
                except:
                    filtered_log['details'] = details_str

            filtered_logs.append(filtered_log)

        return {
            'logs': filtered_logs,
            'total': total_count,
            'limit': limit,
            'offset': offset
        }

    @staticmethod
    def get_statistics(days: int = 30):
        """Get audit log statistics for the admin dashboard."""
        cutoff_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        cutoff_date = (cutoff_date - timedelta(days=days)).isoformat()

        stats = {}

        # Total logs
        result = db.execute_one('SELECT COUNT(*) as count FROM enhanced_audit_log WHERE created_at >= ?', (cutoff_date,))
        stats['total_logs'] = result['count'] if result else 0

        # Logs by action
        rows = db.execute(
            'SELECT action, COUNT(*) as count FROM enhanced_audit_log WHERE created_at >= ? GROUP BY action ORDER BY count DESC',
            (cutoff_date,)
        )
        stats['by_action'] = {row['action']: row['count'] for row in rows}

        # Logs by user
        rows = db.execute(
            'SELECT user_id, COUNT(*) as count FROM enhanced_audit_log WHERE created_at >= ? AND user_id IS NOT NULL GROUP BY user_id ORDER BY count DESC LIMIT 10',
            (cutoff_date,)
        )
        stats['by_user'] = {row['user_id']: row['count'] for row in rows}

        # Failed actions
        result = db.execute_one(
            'SELECT COUNT(*) as count FROM enhanced_audit_log WHERE created_at >= ? AND (status_code >= 400 OR error_message IS NOT NULL)',
            (cutoff_date,)
        )
        stats['failed_actions'] = result['count'] if result else 0

        # Unique IPs
        result = db.execute_one(
            'SELECT COUNT(DISTINCT ip_address) as count FROM enhanced_audit_log WHERE created_at >= ?',
            (cutoff_date,)
        )
        stats['unique_ips'] = result['count'] if result else 0

        # Top countries
        rows = db.execute(
            '''SELECT geo_location, COUNT(*) as count
               FROM enhanced_audit_log
               WHERE created_at >= ? AND geo_location IS NOT NULL
               GROUP BY geo_location
               ORDER BY count DESC LIMIT 10''',
            (cutoff_date,)
        )

        country_counts = {}
        for row in rows:
            try:
                geo = json.loads(row['geo_location'])
                country = geo.get('country', 'Unknown')
                country_counts[country] = country_counts.get(country, 0) + row['count']
            except:
                pass
        stats['by_country'] = country_counts

        return stats


# Global enhanced audit logger instance
enhanced_audit_logger = EnhancedAuditLogger()
